{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "-----------------------------------\n",
    "\n",
    "Here we're training on a pretty large dataset of infected and uninfected images. To download the dataset yourself, you can go [here](https://www.kaggle.com/iarunava/cell-images-for-detecting-malaria/download)\n",
    "\n",
    "## Setting up the dataset\n",
    "\n",
    "Follow the steps below to get your dataset set up.\n",
    " - Extract the downloaded zip\n",
    " - Rename `cell_images` to `train_cell_images`\n",
    " - Create a directory called `model`. This is where the trained model will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras.layers import SeparableConv2D, MaxPooling2D, Dense, Dropout, GlobalMaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load infected images\n",
    "Parasitized = os.listdir(\"./dataset/train_cell_images/Parasitized/\")\n",
    "for p in Parasitized:\n",
    "    try:\n",
    "        image = cv2.imread(\"./dataset/train_cell_images/Parasitized/\" + p)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        rotated45 = size_image.rotate(45)\n",
    "        rotated75 = size_image.rotate(75)\n",
    "        blur = cv2.blur(np.array(size_image), (10, 10))\n",
    "        data.append(np.array(size_image))\n",
    "        data.append(np.array(rotated45))\n",
    "        data.append(np.array(rotated75))\n",
    "        data.append(np.array(blur))\n",
    "        labels.append(0.0)\n",
    "        labels.append(0.0)\n",
    "        labels.append(0.0)\n",
    "        labels.append(0.0)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# Load uninfected images\n",
    "Uninfected = os.listdir(\"./dataset/train_cell_images/Uninfected/\")\n",
    "for u in Uninfected:\n",
    "    try:\n",
    "        image = cv2.imread(\"./dataset/train_cell_images/Uninfected/\" + u)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        rotated45 = size_image.rotate(45)\n",
    "        rotated75 = size_image.rotate(75)\n",
    "        data.append(np.array(size_image))\n",
    "        data.append(np.array(rotated45))\n",
    "        data.append(np.array(rotated75))\n",
    "        labels.append(1.0)\n",
    "        labels.append(1.0)\n",
    "        labels.append(1.0)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# Convert image pixels to numpy arrays for easy processing\n",
    "cells = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "np.save(\"model/cells\", cells)\n",
    "np.save(\"model/labels\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = np.load(\"model/cells.npy\")\n",
    "labels = np.load(\"model/labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72339, 50, 50, 3)\n",
      "(72339,)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle cells to prevent some sort of bias\n",
    "s = np.arange(cells.shape[0])\n",
    "np.random.shuffle(s)\n",
    "cells = cells[s]\n",
    "labels = labels[s]\n",
    "\n",
    "num_classes = len(np.unique(labels))\n",
    "len_data = len(cells)\n",
    "\n",
    "# Split into train and test datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(cells, labels)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255  # Normalize RGB values by dividing with 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "train_len = len(x_train)\n",
    "test_len = len(x_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "separable_conv2d_4 (Separabl (None, 50, 50, 8)         59        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 25, 25, 8)         0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_5 (Separabl (None, 25, 25, 16)        216       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_6 (Separabl (None, 12, 12, 32)        688       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,564\n",
      "Trainable params: 2,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 72339 samples, validate on 24114 samples\n",
      "Epoch 1/20\n",
      "72339/72339 [==============================] - 12s 169us/step - loss: 0.6428 - accuracy: 0.6163 - val_loss: 0.5647 - val_accuracy: 0.6935\n",
      "Epoch 2/20\n",
      "72339/72339 [==============================] - 10s 143us/step - loss: 0.5092 - accuracy: 0.7410 - val_loss: 0.3964 - val_accuracy: 0.8342\n",
      "Epoch 3/20\n",
      "72339/72339 [==============================] - 10s 138us/step - loss: 0.2703 - accuracy: 0.9018 - val_loss: 0.1742 - val_accuracy: 0.9453\n",
      "Epoch 4/20\n",
      "72339/72339 [==============================] - 10s 135us/step - loss: 0.1796 - accuracy: 0.9428 - val_loss: 0.1518 - val_accuracy: 0.9528\n",
      "Epoch 5/20\n",
      "72339/72339 [==============================] - 10s 136us/step - loss: 0.1654 - accuracy: 0.9486 - val_loss: 0.1455 - val_accuracy: 0.9553\n",
      "Epoch 6/20\n",
      "72339/72339 [==============================] - 10s 139us/step - loss: 0.1573 - accuracy: 0.9520 - val_loss: 0.1387 - val_accuracy: 0.9566\n",
      "Epoch 7/20\n",
      "72339/72339 [==============================] - 10s 138us/step - loss: 0.1519 - accuracy: 0.9546 - val_loss: 0.1359 - val_accuracy: 0.9572\n",
      "Epoch 8/20\n",
      "72339/72339 [==============================] - 10s 142us/step - loss: 0.1488 - accuracy: 0.9549 - val_loss: 0.1354 - val_accuracy: 0.9582\n",
      "Epoch 9/20\n",
      "72339/72339 [==============================] - 10s 133us/step - loss: 0.1449 - accuracy: 0.9559 - val_loss: 0.1321 - val_accuracy: 0.9588\n",
      "Epoch 10/20\n",
      "72339/72339 [==============================] - 10s 139us/step - loss: 0.1421 - accuracy: 0.9564 - val_loss: 0.1281 - val_accuracy: 0.9590\n",
      "Epoch 11/20\n",
      "72339/72339 [==============================] - 10s 137us/step - loss: 0.1393 - accuracy: 0.9573 - val_loss: 0.1270 - val_accuracy: 0.9592\n",
      "Epoch 12/20\n",
      "72339/72339 [==============================] - 10s 138us/step - loss: 0.1354 - accuracy: 0.9580 - val_loss: 0.1262 - val_accuracy: 0.9596\n",
      "Epoch 13/20\n",
      "72339/72339 [==============================] - 10s 136us/step - loss: 0.1331 - accuracy: 0.9584 - val_loss: 0.1225 - val_accuracy: 0.9598\n",
      "Epoch 14/20\n",
      "72339/72339 [==============================] - 10s 136us/step - loss: 0.1329 - accuracy: 0.9584 - val_loss: 0.1245 - val_accuracy: 0.9601\n",
      "Epoch 15/20\n",
      "72339/72339 [==============================] - 10s 135us/step - loss: 0.1293 - accuracy: 0.9594 - val_loss: 0.1195 - val_accuracy: 0.9610\n",
      "Epoch 00015: early stopping\n",
      "24114/24114 [==============================] - 2s 91us/step\n",
      "\n",
      " Validation accuracy:  0.9610185027122498\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        min_delta=1e-2,\n",
    "        patience=10,\n",
    "        verbose=1),\n",
    "    TensorBoard(\n",
    "        log_dir='logs',\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    )\n",
    "\n",
    "]\n",
    "\n",
    "# Create a sequential keras model\n",
    "model = Sequential()\n",
    "model.add(SeparableConv2D(filters=8, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=(50, 50, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(SeparableConv2D(filters=16, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(SeparableConv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "\n",
    "# compile the model with loss function as binary_crossentropy and using adam optimizer you can test result by trying\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit (train) the model. Using a batch size which is x^2 optimizes training on my GPU\n",
    "model.fit(x_train, y_train, batch_size=512, epochs=20, verbose=1, callbacks=callbacks, validation_data=(x_test, y_test))\n",
    "\n",
    "accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('\\n', 'Validation accuracy: ', accuracy[1])\n",
    "\n",
    "model.save('model/saved_model.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "------------------\n",
    "\n",
    "## Setting up prediction dataset\n",
    " - Create a directory called `test_cell_images`\n",
    " - Copy any two images of your choosing into this directory. Name them `test1` and `test2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the type of cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted cell is an Infected cell.\n",
      "The predicted cell is an Uninfected cell.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def convert_to_array(img):\n",
    "    im = cv2.imread(img)\n",
    "    img_ = Image.fromarray(im, 'RGB')\n",
    "    image = img_.resize((50, 50))\n",
    "    image = np.array(image)\n",
    "    image = image / 255\n",
    "    return np.array([image], dtype=np.float32)\n",
    "\n",
    "\n",
    "def get_cell_name(label):\n",
    "    if label == 0:\n",
    "        return \"Infected\"\n",
    "    if label == 1:\n",
    "        return \"Uninfected\"\n",
    "\n",
    "\n",
    "def predict_cell(file):\n",
    "    model = load_model('./model/saved_model.pb')\n",
    "    arr = convert_to_array(file)\n",
    "    score = model.predict(arr, verbose=3)\n",
    "    cell = get_cell_name(round(score[0][0]))\n",
    "    return \"The predicted cell is an \" + cell + \" cell.\"\n",
    "\n",
    "\n",
    "print(predict_cell('./dataset/test_cell_images/test1.png')) # Infected\n",
    "print(predict_cell('./dataset/test_cell_images/test2.png')) # Not infected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "-----------------------------------------------\n",
    "\n",
    "Often, we'll want to deploy a model to a mobile device, or we just want to reduce its size. TFLite is a great way to reduce size and make models compatible with mobile devices, without losing much performance and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keras model from disk\n",
    "model = load_model('./model/saved_model.pb')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Create the tflite model file\n",
    "with open('model/cells.tflite', \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted cell is an Uninfected cell.\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model/cells.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "input_img = convert_to_array('./dataset/test_cell_images/test2.png')\n",
    "\n",
    "# Run inference\n",
    "interpreter.set_tensor(input_details[0]['index'], input_img)\n",
    "interpreter.invoke()\n",
    "output = round(interpreter.get_tensor(output_details[0]['index'])[0][0])\n",
    "print(\"The predicted cell is an \" + get_cell_name(output) + \" cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
